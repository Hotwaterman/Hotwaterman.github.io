<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Multi-task reinforcement learning with soft modularization | Gridea</title>
<link rel="shortcut icon" href="https://hotwaterman.github.io/favicon.ico?v=1651917491789">
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.3.0/fonts/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="https://hotwaterman.github.io/styles/main.css">
<link rel="alternate" type="application/atom+xml" title="Multi-task reinforcement learning with soft modularization | Gridea - Atom Feed" href="https://hotwaterman.github.io/atom.xml">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700">



    <meta name="description" content="
在多任务强化学习中，因为任务的性质和难易不同，在利用一个模型同时训练多个任务时，往往会出现相差较大的任务梯度冲突，容易的任务先达到收敛等问题。目前解决这些问题的方法有知识蒸馏（即学多个任务policy之后蒸馏给一个policy），梯度修剪..." />
    <meta name="keywords" content="" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
    <script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
  </head>
  <body>
    <div class="main">
      <div class="main-content">
        <div class="site-header">
  <a href="https://hotwaterman.github.io">
  <img class="avatar" src="https://hotwaterman.github.io/images/avatar.png?v=1651917491789" alt="">
  </a>
  <h1 class="site-title">
    Gridea
  </h1>
  <p class="site-description">
    温故而知新
  </p>
  <div class="menu-container">
    
      
        <a href="/" class="menu">
          首页
        </a>
      
    
      
        <a href="/archives" class="menu">
          归档
        </a>
      
    
      
        <a href="/tags" class="menu">
          标签
        </a>
      
    
      
        <a href="/post/about" class="menu">
          关于
        </a>
      
    
  </div>
  <div class="social-container">
    
      
    
      
    
      
    
      
    
      
    
  </div>
</div>

        <div class="post-detail">
          <article class="post">
            <h2 class="post-title">
              Multi-task reinforcement learning with soft modularization
            </h2>
            <div class="post-info">
              <span>
                2022-05-07
              </span>
              <span>
                3 min read
              </span>
              
            </div>
            
            <div class="post-content-wrapper">
              <div class="post-content">
                <p><img src="https://hotwaterman.github.io/post-images/1651917339812.png" alt="" loading="lazy"><br>
在多任务强化学习中，因为任务的性质和难易不同，在利用一个模型同时训练多个任务时，往往会出现相差较大的任务梯度冲突，容易的任务先达到收敛等问题。目前解决这些问题的方法有知识蒸馏（即学多个任务policy之后蒸馏给一个policy），梯度修剪（即在计算不同任务梯度时按照投影方向更新等），progress网络（好像是增量式的学任务，增量式的加网络），还有pathway类（即本文这种，通过激活不同的模块来增加网络的容量）。<br>
<img src="https://hotwaterman.github.io/post-images/1651917368862.png" alt="" loading="lazy"><br>
<img src="https://hotwaterman.github.io/post-images/1651917374824.png" alt="" loading="lazy"></p>
<h3 id="网络结构">网络结构</h3>
<p>从上面两个图片中的网络结构可以看出，网络方面的主要工作大约有两点：1. 学习一个多层级且每个层级有多个模块的base 网络（这里的一个模块可以是很多层网络，输出也是多维） 2. 学习一个分配不同模块间权重的routing网络 （注意到base网络的输入是通用的环境状态（与任务无关），而routing网络的输入是包含任务特有信息的）。这里也是因为meta world这个任务是在相同环境下相同物理动态信息下的不同任务，所以base网络可以学习基础通用的信息，然后利用routing网络来进行特定task的调整。</p>
<p>这里我也发现将通用信息和特定信息进行分离貌似是当前RL的一种趋势，我觉得可以学习一个像NLP和CV里面那种容量更大的通用模型（可以进行预训练），和一个特定任务的小网络，两者进行加权来finetune到任何一个任务上，不仅速度更快，而且可以充分发挥当前大网络的优势。<br>
<img src="https://hotwaterman.github.io/post-images/1651917396375.png" alt="" loading="lazy"><br>
题目里面的soft是指在routing网络生成的权重分配中是利用softmax进行平滑。</p>
<h3 id="多任务优化">多任务优化</h3>
<p><img src="https://hotwaterman.github.io/post-images/1651917461569.png" alt="" loading="lazy"><br>
这里利用SAC中衡量策略熵的alpha来判断当前任务的难易程度（难得任务更难收敛，所以熵会更大），然后对policy loss里面不同任务进行一个加权处理（类似梯度修剪，VMPO等，就是让简单的任务等等难的）。</p>
<h3 id="实验结果">实验结果</h3>
<p>从实验结果上可以看出效果还是很不错的。<br>
<img src="https://hotwaterman.github.io/post-images/1651917486271.png" alt="" loading="lazy"></p>

              </div>
              <div class="toc-container">
                <ul class="markdownIt-TOC">
<li>
<ul>
<li>
<ul>
<li><a href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84">网络结构</a></li>
<li><a href="#%E5%A4%9A%E4%BB%BB%E5%8A%A1%E4%BC%98%E5%8C%96">多任务优化</a></li>
<li><a href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C">实验结果</a></li>
</ul>
</li>
</ul>
</li>
</ul>

              </div>
            </div>
          </article>
        </div>

        

        

        <div class="site-footer">
  Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a>
  <a class="rss" href="https://hotwaterman.github.io/atom.xml" target="_blank">
    <i class="ri-rss-line"></i> RSS
  </a>
</div>

      </div>
    </div>

    <script>
      hljs.initHighlightingOnLoad()

      let mainNavLinks = document.querySelectorAll(".markdownIt-TOC a");

      // This should probably be throttled.
      // Especially because it triggers during smooth scrolling.
      // https://lodash.com/docs/4.17.10#throttle
      // You could do like...
      // window.addEventListener("scroll", () => {
      //    _.throttle(doThatStuff, 100);
      // });
      // Only not doing it here to keep this Pen dependency-free.

      window.addEventListener("scroll", event => {
        let fromTop = window.scrollY;

        mainNavLinks.forEach((link, index) => {
          let section = document.getElementById(decodeURI(link.hash).substring(1));
          let nextSection = null
          if (mainNavLinks[index + 1]) {
            nextSection = document.getElementById(decodeURI(mainNavLinks[index + 1].hash).substring(1));
          }
          if (section.offsetTop <= fromTop) {
            if (nextSection) {
              if (nextSection.offsetTop > fromTop) {
                link.classList.add("current");
              } else {
                link.classList.remove("current");    
              }
            } else {
              link.classList.add("current");
            }
          } else {
            link.classList.remove("current");
          }
        });
      });

    </script>
  </body>
</html>
